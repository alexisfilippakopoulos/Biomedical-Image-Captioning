{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"sagittal\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "label_csv = os.path.join(base_dir, \"train-abnormal.csv\")\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(label_csv)\n",
    "#print(f\"Total scans: {len(labels_df)}\")\n",
    "# Choose a sample (change index to view other examples)\n",
    "sample_index = 0\n",
    "img_id = str(labels_df.loc[sample_index, '1']).zfill(4)\n",
    "label = labels_df.loc[sample_index, '1']\n",
    "\n",
    "# Load image\n",
    "img_path = os.path.join(image_dir, f\"{img_id}.npy\")\n",
    "scan = np.load(img_path)\n",
    "\n",
    "print(f\"Scan shape: {scan.shape} — Label: {label}\")\n",
    "\n",
    "# Plot a few slices\n",
    "num_slices = scan.shape[0]\n",
    "slices_to_plot = np.linspace(0, num_slices - 1, 6, dtype=int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, slice_idx in enumerate(slices_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(scan[slice_idx], cmap='gray')\n",
    "    plt.title(f\"Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(f\"Sample {img_id} — Abnormal: {label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "abnormal_df = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df.columns = ['exam', 'abnormal']\n",
    "acl_df.columns = ['exam', 'acl']\n",
    "meniscus_df.columns = ['exam', 'meniscus']\n",
    "\n",
    "merged_df = abnormal_df.merge(acl_df, on='exam').merge(meniscus_df, on='exam')\n",
    "\n",
    "\n",
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"The depicted knee appears to be normal.\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"an ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"a meniscus tear\")\n",
    "\n",
    "    if findings:\n",
    "        return \"The depicted knee has \" + \" and \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"The depicted knee has an unspecified abnormality that is neither an acl nor a meniscus tear.\"\n",
    "\n",
    "\n",
    "merged_df['caption'] = merged_df.apply(generate_caption, axis=1)\n",
    "merged_df\n",
    "\n",
    "merged_df[\"caption\"].unique()\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "# Use CLIP preprocessing (from OpenAI or OpenCLIP)\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "def load_exam_mri(path):\n",
    "    scan = np.load(path)  # shape: (slices, H, W)\n",
    "\n",
    "    # Choose middle 3 slices and stack to simulate RGB\n",
    "    mid = scan.shape[0] // 2\n",
    "    slices = scan[mid - 1: mid + 2]\n",
    "\n",
    "    # Normalize to [0, 255] and convert to uint8 for PIL\n",
    "    slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "\n",
    "    # Convert to PIL and preprocess\n",
    "    img = Image.fromarray(slices)\n",
    "    return clip_preprocess(img)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# We will only use model.visual (vision encoder)\n",
    "\n",
    "\n",
    "def encode_image(tensor_image):\n",
    "    tensor_image = tensor_image.unsqueeze(0).to(device)  # add batch dim\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(tensor_image)\n",
    "    return image_embedding  # shape: (1, 512)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Freeze CLIP if you want:\n",
    "for p in model.visual.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(-1, self.prefix_len, self.gpt.config.n_embd)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        # Concatenate projected image embedding with caption tokens\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        # Shift attention mask accordingly\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((captions.shape[0], self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=captions)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for batch in dataloader:\n",
    "    images, captions = batch  # preprocessed tensors\n",
    "    image_embeddings = model.encode_image(images)\n",
    "\n",
    "    tokens = tokenizer(captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = tokens.input_ids.to(device)\n",
    "    attention_mask = tokens.attention_mask.to(device)\n",
    "\n",
    "    loss = clip_gpt_model(image_embeddings, input_ids, attention_mask).loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
