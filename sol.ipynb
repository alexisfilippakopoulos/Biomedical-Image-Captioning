{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install nltk rouge-score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')  # for METEOR\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Force download again (even if already there)\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('wordnet', force=True)\n",
    "nltk.download('omw-1.4', force=True)\n",
    "\n",
    "# âœ… OPTIONAL: Check paths\n",
    "print(nltk.data.path)\n",
    "\n",
    "\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"Healthy knee\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"Meniscus tear\")\n",
    "    if findings:\n",
    "        return \" and a \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"Unspecified abnormality.\"\n",
    "\n",
    "\n",
    "def load_exam_mri(path):\n",
    "    scan = np.load(path)  # shape: (slices, H, W)\n",
    "\n",
    "    # Choose middle 3 slices and stack to simulate RGB\n",
    "    mid = scan.shape[0] // 2\n",
    "    slices = scan[mid - 1: mid + 2]\n",
    "\n",
    "    # Normalize to [0, 255] and convert to uint8 for PIL\n",
    "    slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "\n",
    "    # Convert to PIL and preprocess\n",
    "    img = Image.fromarray(slices)\n",
    "    return clip_preprocess(img)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        batch_size = captions.shape[0]\n",
    "\n",
    "        # ðŸ’¡ Cast to float32 to match the Linear layer's weights\n",
    "        image_embedding = image_embedding.float()\n",
    "\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        labels = torch.cat((\n",
    "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
    "            captions\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def encode_image(tensor_image):\n",
    "    tensor_image = tensor_image.unsqueeze(0).to(device)  # add batch dim\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(tensor_image)\n",
    "    return image_embedding  # shape: (1, 512)\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MRICaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50, num_slices_to_use=5):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_slices_to_use = num_slices_to_use\n",
    "        self.clip_dim = 512  # CLIP's embedding dimension\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        exam_id = str(row['exam']).zfill(4)\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
    "\n",
    "        # Load all slices from the scan\n",
    "        scan = np.load(img_path)  # shape: (slices, H, W)\n",
    "        num_slices = scan.shape[0]\n",
    "        \n",
    "        # Select evenly spaced slices throughout the volume\n",
    "        if num_slices <= self.num_slices_to_use:\n",
    "            selected_slices = range(num_slices)\n",
    "        else:\n",
    "            selected_slices = np.linspace(0, num_slices-1, self.num_slices_to_use, dtype=int)\n",
    "            \n",
    "        # Take the middle slice for dataset output (representative slice)\n",
    "        mid_slice = num_slices // 2\n",
    "        slice_img = scan[mid_slice]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = self.transform(pil_img)\n",
    "\n",
    "        # Tokenize caption with explicit attention mask\n",
    "        tokens = self.tokenizer(\n",
    "            caption, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True,\n",
    "            max_length=self.max_length, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        # Return representative slice tensor and metadata\n",
    "        return img_tensor, input_ids, attention_mask, exam_id, selected_slices.tolist() if hasattr(selected_slices, 'tolist') else list(selected_slices)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_masks, exam_ids, selected_slices = zip(*batch)\n",
    "    \n",
    "    # Create proper tensors\n",
    "    images_tensor = torch.stack(images)\n",
    "    input_ids_tensor = torch.stack(input_ids)\n",
    "    attention_masks_tensor = torch.stack(attention_masks)\n",
    "    \n",
    "    return images_tensor, input_ids_tensor, attention_masks_tensor, exam_ids, selected_slices\n",
    "\n",
    "# Memory-efficient slice encoding function\n",
    "def encode_selected_slices(clip_model, exam_id, selected_slices, image_dir, transform, device):\n",
    "    \"\"\"Encodes selected slices of an MRI exam and aggregates them\"\"\"\n",
    "    img_path = os.path.join(image_dir, f\"{exam_id}.npy\")\n",
    "    scan = np.load(img_path)\n",
    "    \n",
    "    # Process each selected slice and get embedding\n",
    "    slice_embeddings = []\n",
    "    for slice_idx in selected_slices:\n",
    "        # Convert slice to 3-channel image\n",
    "        slice_img = scan[slice_idx]\n",
    "        slice_img = ((slice_img - slice_img.min()) / (slice_img.max() - slice_img.min()) * 255).astype(np.uint8)\n",
    "        slice_rgb = np.stack([slice_img, slice_img, slice_img], axis=-1)\n",
    "        pil_img = Image.fromarray(slice_rgb)\n",
    "        img_tensor = transform(pil_img).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.encode_image(img_tensor)\n",
    "        slice_embeddings.append(embedding)\n",
    "    \n",
    "    # Stack embeddings and take mean across slices\n",
    "    all_embeddings = torch.cat(slice_embeddings, dim=0)\n",
    "    mean_embedding = torch.mean(all_embeddings, dim=0, keepdim=True)\n",
    "    \n",
    "    return mean_embedding\n",
    "\n",
    "# Improved training function with proper error handling\n",
    "def train_epoch(caption_model, clip_model, dataloader, optimizer, device, image_dir, transform):\n",
    "    caption_model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        try:\n",
    "            images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            \n",
    "            # Process selected slices for each exam in batch\n",
    "            batch_embeddings = []\n",
    "            for i in range(len(exam_ids)):\n",
    "                exam_embedding = encode_selected_slices(\n",
    "                    clip_model,\n",
    "                    exam_ids[i],\n",
    "                    selected_slices[i],\n",
    "                    image_dir,\n",
    "                    transform,\n",
    "                    device\n",
    "                )\n",
    "                batch_embeddings.append(exam_embedding)\n",
    "            \n",
    "            # Stack embeddings into batch\n",
    "            image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "            \n",
    "            # Forward pass through caption model\n",
    "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Perform backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / max(1, batch_count)\n",
    "\n",
    "def evaluate(caption_model, clip_model, dataloader, device, tokenizer, image_dir, transform):\n",
    "    caption_model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Tracking predicted and true captions\n",
    "    true_caption_counts = {}  # Dictionary to count true captions\n",
    "    pred_caption_counts = {}  # Dictionary to count predicted captions\n",
    "    caption_pairs = []  # List to store (true, pred) caption pairs for analysis\n",
    "    \n",
    "    # Metrics\n",
    "    all_bleu_scores = []\n",
    "    all_rouge_scores = []\n",
    "    \n",
    "    smooth = SmoothingFunction().method4\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            try:\n",
    "                images, input_ids, attention_mask, exam_ids, selected_slices = batch\n",
    "                input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "                \n",
    "                # Process selected slices for each exam in batch\n",
    "                batch_embeddings = []\n",
    "                for i in range(len(exam_ids)):\n",
    "                    exam_embedding = encode_selected_slices(\n",
    "                        clip_model,\n",
    "                        exam_ids[i],\n",
    "                        selected_slices[i],\n",
    "                        image_dir,\n",
    "                        transform,\n",
    "                        device\n",
    "                    )\n",
    "                    batch_embeddings.append(exam_embedding)\n",
    "                \n",
    "                # Stack embeddings into batch\n",
    "                image_embeddings = torch.cat(batch_embeddings, dim=0)\n",
    "                \n",
    "                # Forward pass for loss calculation\n",
    "                outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Generate captions for each image in batch\n",
    "                batch_size = input_ids.size(0)\n",
    "                for i in range(batch_size):\n",
    "                    # Get ground truth caption\n",
    "                    true_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Update true caption counts\n",
    "                    if true_caption in true_caption_counts:\n",
    "                        true_caption_counts[true_caption] += 1\n",
    "                    else:\n",
    "                        true_caption_counts[true_caption] = 1\n",
    "                    \n",
    "                    # Generate predicted caption\n",
    "                    prefix_embed = caption_model.clip_project(image_embeddings[i].float().unsqueeze(0)) \\\n",
    "                        .view(1, caption_model.prefix_len, -1)\n",
    "                    \n",
    "                    # Generate with attention mask for beam search\n",
    "                    attention_prefix = torch.ones(1, caption_model.prefix_len, device=device)\n",
    "                    \n",
    "                    generated = caption_model.gpt.generate(\n",
    "                        inputs_embeds=prefix_embed,\n",
    "                        max_length=50,\n",
    "                        num_beams=5,\n",
    "                        early_stopping=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                        attention_mask=attention_prefix\n",
    "                    )\n",
    "                    pred_caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Update predicted caption counts\n",
    "                    if pred_caption in pred_caption_counts:\n",
    "                        pred_caption_counts[pred_caption] += 1\n",
    "                    else:\n",
    "                        pred_caption_counts[pred_caption] = 1\n",
    "                    \n",
    "                    # Store the pair for later analysis\n",
    "                    caption_pairs.append((true_caption, pred_caption))\n",
    "                    \n",
    "                    # Calculate BLEU score\n",
    "                    reference = [nltk.word_tokenize(true_caption.lower())]\n",
    "                    candidate = nltk.word_tokenize(pred_caption.lower())\n",
    "                    if len(candidate) > 0:  # Check if candidate has words\n",
    "                        bleu = sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
    "                        all_bleu_scores.append(bleu)\n",
    "                    \n",
    "                    # Calculate ROUGE score\n",
    "                    rouge = scorer.score(true_caption, pred_caption)['rougeL'].fmeasure\n",
    "                    all_rouge_scores.append(rouge)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in validation batch: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Calculate standard metrics\n",
    "    avg_loss = total_loss / max(1, len(dataloader))\n",
    "    avg_bleu = sum(all_bleu_scores) / max(1, len(all_bleu_scores))\n",
    "    avg_rouge = sum(all_rouge_scores) / max(1, len(all_rouge_scores))\n",
    "    \n",
    "    # Analyze caption distribution\n",
    "    total_samples = len(caption_pairs)\n",
    "    unique_pred_captions = len(pred_caption_counts)\n",
    "    unique_true_captions = len(true_caption_counts)\n",
    "    \n",
    "    # Create a mapping of how often each predicted caption matched with each true caption\n",
    "    pred_to_true_map = {}\n",
    "    for true_cap, pred_cap in caption_pairs:\n",
    "        if pred_cap not in pred_to_true_map:\n",
    "            pred_to_true_map[pred_cap] = {}\n",
    "        \n",
    "        if true_cap not in pred_to_true_map[pred_cap]:\n",
    "            pred_to_true_map[pred_cap][true_cap] = 1\n",
    "        else:\n",
    "            pred_to_true_map[pred_cap][true_cap] += 1\n",
    "    \n",
    "    # Sort captions by frequency\n",
    "    sorted_pred_captions = sorted(pred_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_true_captions = sorted(true_caption_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Report on caption distribution\n",
    "    caption_analysis = {\n",
    "        \"total_samples\": total_samples,\n",
    "        \"unique_predicted_captions\": unique_pred_captions,\n",
    "        \"unique_true_captions\": unique_true_captions,\n",
    "        \"top_predicted_captions\": sorted_pred_captions[:10],  # Top 10 most frequent predictions\n",
    "        \"top_true_captions\": sorted_true_captions[:10],       # Top 10 most frequent ground truths\n",
    "        \"prediction_to_true_map\": pred_to_true_map,           # Mapping of predictions to ground truths\n",
    "        \"repetition_rate\": 1 - (unique_pred_captions / total_samples)  # Higher means more repetition\n",
    "    }\n",
    "    for key, value in caption_analysis.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    return avg_loss, avg_bleu, avg_rouge\n",
    "\n",
    "def init_model(device):\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    \n",
    "    # Freeze CLIP parameters \n",
    "    for param in clip_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Configure tokenizer with proper padding\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create caption model\n",
    "    caption_model = ClipCaptionModel(clip_dim=512, prefix_len=10).to(device)\n",
    "    \n",
    "    # Use proper optimizer with weight decay\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        caption_model.parameters(),\n",
    "        lr=2e-5,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    return clip_model, preprocess, tokenizer, caption_model, optimizer\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"sagittal\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "label_csv = os.path.join(base_dir, \"train-abnormal.csv\")\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "\n",
    "\n",
    "abnormal_df = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df.columns = ['exam', 'abnormal']\n",
    "acl_df.columns = ['exam', 'acl']\n",
    "meniscus_df.columns = ['exam', 'meniscus']\n",
    "\n",
    "df = abnormal_df.merge(acl_df, on='exam').merge(meniscus_df, on='exam')\n",
    "\n",
    "\n",
    "df['caption'] = df.apply(generate_caption, axis=1)\n",
    "df\n",
    "\n",
    "df[\"caption\"].value_counts()\n",
    "\n",
    "acl_meniscus_mask = (df['acl'] == 1) & (df['meniscus'] == 1)\n",
    "acl_only_mask     = (df['acl'] == 1) & (df['meniscus'] == 0)\n",
    "meniscus_only_mask = (df['acl'] == 0) & (df['meniscus'] == 1)\n",
    "healthy_mask      = (df['abnormal'] == 0)\n",
    "unspecified_mask  = (df['abnormal'] == 1) & (df['acl'] == 0) & (df['meniscus'] == 0)\n",
    "\n",
    "# Sample 83 from each group\n",
    "df_acl_meniscus = df[acl_meniscus_mask].sample(n=83, random_state=42)\n",
    "df_acl_only     = df[acl_only_mask].sample(n=83, random_state=42)\n",
    "df_meniscus     = df[meniscus_only_mask].sample(n=83, random_state=42)\n",
    "df_healthy      = df[healthy_mask].sample(n=83, random_state=42)\n",
    "df_unspecified  = df[unspecified_mask].sample(n=83, random_state=42)\n",
    "\n",
    "# Concatenate them\n",
    "df_balanced = pd.concat([\n",
    "    df_acl_meniscus,\n",
    "    df_acl_only,\n",
    "    df_meniscus,\n",
    "    df_healthy,\n",
    "    df_unspecified\n",
    "], ignore_index=True).sample(frac=1, random_state=42)\n",
    "\n",
    "df_balanced[\"caption\"].value_counts()\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "# Use CLIP preprocessing (from OpenAI or OpenCLIP)\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Main training loop\n",
    "def train_model(num_epochs=25, batch_size=8):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Initialize models and data\n",
    "    clip_model, preprocess, tokenizer, caption_model, optimizer = init_model(device)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_df, val_df = train_test_split(df_balanced, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # Select a balanced number of slices from each MRI\n",
    "    train_dataset = MRICaptionDataset(train_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "    val_dataset = MRICaptionDataset(val_df, image_dir, preprocess, tokenizer, num_slices_to_use=10)\n",
    "    \n",
    "    # Create data loaders with custom collate\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    # Initialize best metrics for model checkpointing\n",
    "    best_rouge = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nðŸŒŸ Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train and evaluate\n",
    "        avg_train_loss = train_epoch(caption_model, clip_model, train_loader, optimizer, device, image_dir, preprocess)\n",
    "        avg_val_loss, avg_bleu, avg_rouge = evaluate(\n",
    "            caption_model, clip_model, val_loader, device, tokenizer, image_dir, preprocess\n",
    "        )\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"BLEU: {avg_bleu:.4f} | ROUGE-L: {avg_rouge:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_rouge > best_rouge:\n",
    "            best_rouge = avg_rouge\n",
    "            torch.save(caption_model.state_dict(), f\"/content/drive/MyDrive/biodata Project/best_model.pt\")\n",
    "\n",
    "train_model(num_epochs=25, batch_size=8)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
