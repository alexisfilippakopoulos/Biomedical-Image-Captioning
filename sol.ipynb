{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"sagittal\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "label_csv = os.path.join(base_dir, \"train-abnormal.csv\")\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(label_csv)\n",
    "#print(f\"Total scans: {len(labels_df)}\")\n",
    "# Choose a sample (change index to view other examples)\n",
    "sample_index = 0\n",
    "img_id = str(labels_df.loc[sample_index, '1']).zfill(4)\n",
    "label = labels_df.loc[sample_index, '1']\n",
    "\n",
    "# Load image\n",
    "img_path = os.path.join(image_dir, f\"{img_id}.npy\")\n",
    "scan = np.load(img_path)\n",
    "\n",
    "print(f\"Scan shape: {scan.shape} — Label: {label}\")\n",
    "\n",
    "# Plot a few slices\n",
    "num_slices = scan.shape[0]\n",
    "slices_to_plot = np.linspace(0, num_slices - 1, 6, dtype=int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, slice_idx in enumerate(slices_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(scan[slice_idx], cmap='gray')\n",
    "    plt.title(f\"Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(f\"Sample {img_id} — Abnormal: {label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "abnormal_df = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df.columns = ['exam', 'abnormal']\n",
    "acl_df.columns = ['exam', 'acl']\n",
    "meniscus_df.columns = ['exam', 'meniscus']\n",
    "\n",
    "merged_df = abnormal_df.merge(acl_df, on='exam').merge(meniscus_df, on='exam')\n",
    "\n",
    "\n",
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"The depicted knee appears to be normal.\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"an ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"a meniscus tear\")\n",
    "\n",
    "    if findings:\n",
    "        return \"The depicted knee has \" + \" and \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"The depicted knee has an unspecified abnormality that is neither an acl nor a meniscus tear.\"\n",
    "\n",
    "\n",
    "merged_df['caption'] = merged_df.apply(generate_caption, axis=1)\n",
    "merged_df\n",
    "\n",
    "merged_df[\"caption\"].unique()\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "# Use CLIP preprocessing (from OpenAI or OpenCLIP)\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "def load_exam_mri(path):\n",
    "    scan = np.load(path)  # shape: (slices, H, W)\n",
    "\n",
    "    # Choose middle 3 slices and stack to simulate RGB\n",
    "    mid = scan.shape[0] // 2\n",
    "    slices = scan[mid - 1: mid + 2]\n",
    "\n",
    "    # Normalize to [0, 255] and convert to uint8 for PIL\n",
    "    slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "\n",
    "    # Convert to PIL and preprocess\n",
    "    img = Image.fromarray(slices)\n",
    "    return clip_preprocess(img)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# We will only use model.visual (vision encoder)\n",
    "\n",
    "\n",
    "def encode_image(tensor_image):\n",
    "    tensor_image = tensor_image.unsqueeze(0).to(device)  # add batch dim\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(tensor_image)\n",
    "    return image_embedding  # shape: (1, 512)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Freeze CLIP if you want:\n",
    "for p in model.visual.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        batch_size = captions.shape[0]\n",
    "\n",
    "        # 💡 Cast to float32 to match the Linear layer's weights\n",
    "        image_embedding = image_embedding.float()\n",
    "\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        labels = torch.cat((\n",
    "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
    "            captions\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MRICaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        exam_id = str(row['exam']).zfill(4)\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
    "\n",
    "        # Load and process image\n",
    "        scan = np.load(img_path)\n",
    "        mid = scan.shape[0] // 2\n",
    "        slices = scan[mid - 1: mid + 2]\n",
    "        slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "        img = Image.fromarray(slices)\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", truncation=True,\n",
    "                                max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        return img_tensor, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MRICaptionDataset(train_df, image_dir, clip_preprocess, tokenizer)\n",
    "val_dataset = MRICaptionDataset(val_df, image_dir, clip_preprocess, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "clip_gpt_model = ClipCaptionModel().to(device)\n",
    "optimizer = torch.optim.AdamW(clip_gpt_model.parameters(), lr=1e-4)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def encode_clip_images(model, images):\n",
    "    \"\"\"Encodes a batch of images using CLIP's visual encoder.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.encode_image(images.to(device))\n",
    "    return image_embeddings\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        image_embeddings = encode_clip_images(model, images)\n",
    "\n",
    "        outputs = model(image_embeddings, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            image_embeddings = encode_clip_images(model, images)\n",
    "            outputs = model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def encode_clip_images(clip_model, images):\n",
    "    \"\"\"Encodes a batch of images using CLIP's visual encoder.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        return clip_model.encode_image(images.to(device))\n",
    "\n",
    "def train_epoch(caption_model, clip_model, dataloader, optimizer, device):\n",
    "    caption_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        image_embeddings = encode_clip_images(clip_model, images)\n",
    "        outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(caption_model, clip_model, dataloader, device):\n",
    "    caption_model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "            image_embeddings = encode_clip_images(clip_model, images)\n",
    "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(f\"\\n🌟 Epoch {epoch + 1}/{10}\")\n",
    "\n",
    "    train_loss = train_epoch(clip_gpt_model, model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(clip_gpt_model, model, val_loader, device)\n",
    "\n",
    "    print(f\"✅ Train Loss: {train_loss:.4f} | 🔍 Val Loss: {val_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
