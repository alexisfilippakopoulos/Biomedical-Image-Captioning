{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install nltk rouge-score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')  # for METEOR\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Force download again (even if already there)\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('wordnet', force=True)\n",
    "nltk.download('omw-1.4', force=True)\n",
    "\n",
    "# ✅ OPTIONAL: Check paths\n",
    "print(nltk.data.path)\n",
    "\n",
    "\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import clip\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "base_dir = \"/content/drive/MyDrive/biodata Project/MRNet-v1.0\"\n",
    "plane = \"sagittal\"  # can be 'axial', 'coronal', 'sagittal'\n",
    "label_csv = os.path.join(base_dir, \"train-abnormal.csv\")\n",
    "image_dir = os.path.join(base_dir, \"train\", plane)\n",
    "\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(label_csv)\n",
    "#print(f\"Total scans: {len(labels_df)}\")\n",
    "# Choose a sample (change index to view other examples)\n",
    "sample_index = 0\n",
    "img_id = str(labels_df.loc[sample_index, '1']).zfill(4)\n",
    "label = labels_df.loc[sample_index, '1']\n",
    "\n",
    "# Load image\n",
    "img_path = os.path.join(image_dir, f\"{img_id}.npy\")\n",
    "scan = np.load(img_path)\n",
    "\n",
    "print(f\"Scan shape: {scan.shape} — Label: {label}\")\n",
    "\n",
    "# Plot a few slices\n",
    "num_slices = scan.shape[0]\n",
    "slices_to_plot = np.linspace(0, num_slices - 1, 6, dtype=int)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, slice_idx in enumerate(slices_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(scan[slice_idx], cmap='gray')\n",
    "    plt.title(f\"Slice {slice_idx}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(f\"Sample {img_id} — Abnormal: {label}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "abnormal_df = pd.read_csv(os.path.join(base_dir, \"train-abnormal.csv\"))\n",
    "acl_df = pd.read_csv(os.path.join(base_dir, \"train-acl.csv\"))\n",
    "meniscus_df = pd.read_csv(os.path.join(base_dir, \"train-meniscus.csv\"))\n",
    "\n",
    "abnormal_df.columns = ['exam', 'abnormal']\n",
    "acl_df.columns = ['exam', 'acl']\n",
    "meniscus_df.columns = ['exam', 'meniscus']\n",
    "\n",
    "merged_df = abnormal_df.merge(acl_df, on='exam').merge(meniscus_df, on='exam')\n",
    "\n",
    "\n",
    "def generate_caption(row):\n",
    "    if row['abnormal'] == 0:\n",
    "        return \"The depicted knee appears to be healthy.\"\n",
    "\n",
    "    findings = []\n",
    "    if row['acl'] == 1:\n",
    "        findings.append(\"an ACL tear\")\n",
    "    if row['meniscus'] == 1:\n",
    "        findings.append(\"a meniscus tear\")\n",
    "\n",
    "    if findings:\n",
    "        return \"The depicted knee has \" + \" and \".join(findings) + \".\"\n",
    "    else:\n",
    "        return \"The depicted knee has an unspecified abnormality.\"\n",
    "\n",
    "\n",
    "merged_df['caption'] = merged_df.apply(generate_caption, axis=1)\n",
    "merged_df\n",
    "\n",
    "merged_df[\"caption\"].unique()\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "# Use CLIP preprocessing (from OpenAI or OpenCLIP)\n",
    "clip_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "def load_exam_mri(path):\n",
    "    scan = np.load(path)  # shape: (slices, H, W)\n",
    "\n",
    "    # Choose middle 3 slices and stack to simulate RGB\n",
    "    mid = scan.shape[0] // 2\n",
    "    slices = scan[mid - 1: mid + 2]\n",
    "\n",
    "    # Normalize to [0, 255] and convert to uint8 for PIL\n",
    "    slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "\n",
    "    # Convert to PIL and preprocess\n",
    "    img = Image.fromarray(slices)\n",
    "    return clip_preprocess(img)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# We will only use model.visual (vision encoder)\n",
    "\n",
    "\n",
    "def encode_image(tensor_image):\n",
    "    tensor_image = tensor_image.unsqueeze(0).to(device)  # add batch dim\n",
    "    with torch.no_grad():\n",
    "        image_embedding = model.encode_image(tensor_image)\n",
    "    return image_embedding  # shape: (1, 512)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "# Freeze CLIP if you want:\n",
    "for p in model.visual.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "    def __init__(self, clip_dim=512, prefix_len=10):\n",
    "        super().__init__()\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.prefix_len = prefix_len\n",
    "        self.clip_project = nn.Linear(clip_dim, self.gpt.config.n_embd * prefix_len)\n",
    "\n",
    "    def forward(self, image_embedding, captions, attention_mask):\n",
    "        batch_size = captions.shape[0]\n",
    "\n",
    "        # 💡 Cast to float32 to match the Linear layer's weights\n",
    "        image_embedding = image_embedding.float()\n",
    "\n",
    "        prefix_embedding = self.clip_project(image_embedding).view(batch_size, self.prefix_len, -1)\n",
    "        caption_embeddings = self.gpt.transformer.wte(captions)\n",
    "\n",
    "        embeddings = torch.cat((prefix_embedding, caption_embeddings), dim=1)\n",
    "\n",
    "        extended_attention = torch.cat((\n",
    "            torch.ones((batch_size, self.prefix_len), device=attention_mask.device),\n",
    "            attention_mask\n",
    "        ), dim=1)\n",
    "\n",
    "        labels = torch.cat((\n",
    "            torch.full((batch_size, self.prefix_len), -100, device=captions.device),\n",
    "            captions\n",
    "        ), dim=1)\n",
    "\n",
    "        outputs = self.gpt(inputs_embeds=embeddings, attention_mask=extended_attention, labels=labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MRICaptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform, tokenizer, max_length=50):\n",
    "        self.data = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        exam_id = str(row['exam']).zfill(4)\n",
    "        caption = row['caption']\n",
    "        img_path = os.path.join(self.image_dir, f\"{exam_id}.npy\")\n",
    "\n",
    "        # Load and process image\n",
    "        scan = np.load(img_path)\n",
    "        mid = scan.shape[0] // 2\n",
    "        slices = scan[mid - 1: mid + 2]\n",
    "        slices = np.stack([((s - s.min()) / (s.max() - s.min()) * 255).astype(np.uint8) for s in slices], axis=-1)\n",
    "        img = Image.fromarray(slices)\n",
    "        img_tensor = self.transform(img)\n",
    "\n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer(caption, padding=\"max_length\", truncation=True,\n",
    "                                max_length=self.max_length, return_tensors=\"pt\")\n",
    "        input_ids = tokens.input_ids.squeeze(0)\n",
    "        attention_mask = tokens.attention_mask.squeeze(0)\n",
    "\n",
    "        return img_tensor, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = MRICaptionDataset(train_df, image_dir, clip_preprocess, tokenizer)\n",
    "val_dataset = MRICaptionDataset(val_df, image_dir, clip_preprocess, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "clip_gpt_model = ClipCaptionModel().to(device)\n",
    "optimizer = torch.optim.AdamW(clip_gpt_model.parameters(), lr=1e-4)\n",
    "\n",
    "def caption_to_vector(caption):\n",
    "    vector = [0, 0, 0]  # [ACL, Meniscus, Abnormality]\n",
    "\n",
    "    if 'ACL tear' in caption:\n",
    "        vector[0] = 1\n",
    "    if 'meniscus tear' in caption:\n",
    "        vector[1] = 1\n",
    "    if 'unspecified abnormality' in caption:\n",
    "        vector[2] = 1\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def encode_clip_images(clip_model, images):\n",
    "    \"\"\"Encodes a batch of images using CLIP's visual encoder.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        return clip_model.encode_image(images.to(device))\n",
    "\n",
    "def train_epoch(caption_model, clip_model, dataloader, optimizer, device):\n",
    "    caption_model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
    "\n",
    "        image_embeddings = encode_clip_images(clip_model, images)\n",
    "        outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate(caption_model, clip_model, dataloader, device, tokenizer):\n",
    "    caption_model.eval()\n",
    "    total_loss = 0\n",
    "    total_bleu = 0\n",
    "    total_rouge_l = 0\n",
    "\n",
    "    smooth = SmoothingFunction().method4\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    true_vectors = []\n",
    "    pred_vectors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, input_ids, attention_mask in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            image_embeddings = encode_clip_images(clip_model, images)\n",
    "            outputs = caption_model(image_embeddings, input_ids, attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            batch_size = input_ids.size(0)\n",
    "            for i in range(batch_size):\n",
    "                # Ground truth\n",
    "                true_caption = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "\n",
    "                # Generate predicted caption\n",
    "                prefix_embed = caption_model.clip_project(image_embeddings[i].float().unsqueeze(0)) \\\n",
    "                    .view(1, caption_model.prefix_len, -1)\n",
    "\n",
    "                generated = caption_model.gpt.generate(\n",
    "                    inputs_embeds=prefix_embed,\n",
    "                    max_length=50,\n",
    "                    num_beams=5,\n",
    "                    early_stopping=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "                pred_caption = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "                # BLEU\n",
    "                reference = nltk.word_tokenize(true_caption)\n",
    "                candidate = nltk.word_tokenize(pred_caption)\n",
    "                bleu = sentence_bleu([reference], candidate, smoothing_function=smooth)\n",
    "                total_bleu += bleu\n",
    "\n",
    "                # ROUGE-L\n",
    "                rouge_l = scorer.score(true_caption, pred_caption)['rougeL'].fmeasure\n",
    "                total_rouge_l += rouge_l\n",
    "\n",
    "                # Soft label vectors\n",
    "                true_vec = caption_to_vector(true_caption)\n",
    "                pred_vec = caption_to_vector(pred_caption)\n",
    "                true_vectors.append(true_vec)\n",
    "                pred_vectors.append(pred_vec)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_bleu = total_bleu / len(dataloader.dataset)\n",
    "    avg_rouge_l = total_rouge_l / len(dataloader.dataset)\n",
    "\n",
    "    # Classification metrics\n",
    "    true_vectors = np.array(true_vectors)\n",
    "    pred_vectors = np.array(pred_vectors)\n",
    "\n",
    "    accuracy = accuracy_score(true_vectors, pred_vectors)\n",
    "    precision = precision_score(true_vectors, pred_vectors, average='micro')\n",
    "    recall = recall_score(true_vectors, pred_vectors, average='micro')\n",
    "    f1 = f1_score(true_vectors, pred_vectors, average='micro')\n",
    "\n",
    "    return avg_loss, avg_bleu, avg_rouge_l, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "for epoch in range(25):\n",
    "    print(f\"\\n🌟 Epoch {epoch + 1}/{25}\")\n",
    "\n",
    "    avg_train_loss = train_epoch(clip_gpt_model, model, train_loader, optimizer, device)\n",
    "    avg_val_loss, avg_bleu, avg_rouge_l, accuracy, precision, recall, f1 = evaluate(clip_gpt_model, model, val_loader, device, tokenizer)\n",
    "    \n",
    "    print(f\"✅ Train Loss: {avg_train_loss: .4f} | 🔍 Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"📝 BLEU: {avg_bleu:.4f}| 📊 ROUGE-L: {avg_rouge_l:.4f}\")\n",
    "    print(f\"🟢 Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"🟡 Precision:       {precision:.4f}\")\n",
    "    print(f\"🔵 Recall:          {recall:.4f}\")\n",
    "    print(f\"🟣 F1 Score:        {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
